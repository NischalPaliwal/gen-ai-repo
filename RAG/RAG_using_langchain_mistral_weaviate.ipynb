{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install weaviate-client langchain langchain-community tiktoken pypdf rapidocr-onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBg9krkgZRWX",
        "outputId": "4ae27650-146d-4a3b-f137-e2c70a31307a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: weaviate-client in /usr/local/lib/python3.12/dist-packages (4.19.2)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.6.0)\n",
            "Requirement already satisfied: rapidocr-onnxruntime in /usr/local/lib/python3.12/dist-packages (1.4.4)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (0.28.1)\n",
            "Requirement already satisfied: validators<1.0.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (0.35.0)\n",
            "Requirement already satisfied: authlib<2.0.0,>=1.6.5 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (1.6.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.12.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (2.12.3)\n",
            "Requirement already satisfied: grpcio<1.80.0,>=1.59.5 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (1.76.0)\n",
            "Requirement already satisfied: protobuf<7.0.0,>=4.21.6 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (5.29.5)\n",
            "Requirement already satisfied: deprecation<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (2.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.7)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.59)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: pyclipper>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (1.4.0)\n",
            "Requirement already satisfied: opencv-python>=4.5.1.48 in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (4.12.0.88)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (1.17.0)\n",
            "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (2.1.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (11.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (1.23.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from authlib<2.0.0,>=1.6.5->weaviate-client) (43.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation<3.0.0,>=2.1.0->weaviate-client) (25.0)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<1.80.0,>=1.59.5->weaviate-client) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.26.0->weaviate-client) (0.16.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (25.9.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.12.0->weaviate-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.12.0->weaviate-client) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.12.0->weaviate-client) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime) (10.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->authlib<2.0.0,>=1.6.5->weaviate-client) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.6.5->weaviate-client) (2.23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqq langchain-weaviate langchain-huggingface"
      ],
      "metadata": {
        "id": "wX-Y7ho2a7wy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_weaviate import WeaviateVectorStore\n",
        "import weaviate\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "weaviate_url = \"vup3o4psmqh9ogassfmrg.c0.asia-southeast1.gcp.weaviate.cloud\"\n",
        "weaviate_api_key = \"WWFnSnVvTXRPWnAwNk9lbl9tMi81WUVKWTU3NzdYaUNHOHRoRmwxTm0xS2JvcStjc0d4TC9hdGpqNEI0PV92MjAw\"\n",
        "\n",
        "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=weaviate_url,\n",
        "    auth_credentials=weaviate.auth.AuthApiKey(api_key=weaviate_api_key)\n",
        ")\n",
        "\n",
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "gAjtnK-l-7w9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8FZbg3oCd5G",
        "outputId": "1a6c2429-4650-47b2-c432-f4d9c9d7b113"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-text-splitters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiYAIOHJIdfE",
        "outputId": "23872880-e4d5-4268-84ac-3ba56ffc7111"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-text-splitters) (1.2.7)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.12.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(file_path=\"/content/2312.05934v3.pdf\", extract_images=True)\n",
        "\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "kcSSj6dJGKaF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "\n",
        "docs = text_splitter.split_documents(documents=documents)"
      ],
      "metadata": {
        "id": "IXc1ngN2III6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_weaviate import WeaviateVectorStore\n",
        "\n",
        "for doc in docs:\n",
        "    if hasattr(doc, 'metadata') and doc.metadata:\n",
        "        cleaned_metadata = {}\n",
        "        for key, value in doc.metadata.items():\n",
        "            clean_key = key.replace('.', '_')\n",
        "            cleaned_metadata[clean_key] = value\n",
        "        doc.metadata = cleaned_metadata\n",
        "\n",
        "db = WeaviateVectorStore.from_documents(docs, embeddings, client=weaviate_client)"
      ],
      "metadata": {
        "id": "X1VAYkJfJg2V"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(db.similarity_search(query=\"What is fine-tuning?\", k=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "501ZyVUN1Lr-",
        "outputId": "eed5944c-5d8d-4e90-ca9b-6d9e13f851c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'creator': 'LaTeX with hyperref', 'total_pages': 14.0, 'page_label': '3', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2312.05934v3.pdf', 'moddate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'producer': 'pdfTeX-1.40.25', 'page': 2.0, 'title': '', 'author': ''}, page_content='a relevant auxiliary knowledge base BQ. Our objective is to\\ndiscover a transformation, denoted as F, that, when applied,\\nwould enhance the knowledge about Q:\\nM′ := F (M, BQ) s.t. LM′,Q > LM,Q. (3)\\nIn this work, we aim to compare two choices for F: fine-\\ntuning and RAG to see which option performs better in this\\nproblem.\\n3.2. Fine-Tuning\\nFine-tuning is the process of adjusting a pre-trained model\\non a specific, often narrower, dataset or task to enhance\\nits performance in that particular domain. Here, it is vital\\nto distinguish between different types of fine-tuning. FT\\ntechniques are commonly classified into supervised, unsu-\\n3'), Document(metadata={'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'creator': 'LaTeX with hyperref', 'total_pages': 14.0, 'page_label': '7', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'source': '/content/2312.05934v3.pdf', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'moddate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'title': '', 'page': 6.0, 'producer': 'pdfTeX-1.40.25', 'author': ''}, page_content='L = 0.25. This can\\npartially be explained by the models using reasoning and/or\\npre-existing knowledge when answering questions that are\\nnot independent of the past information. Some examples of\\nthis can be found in Appendix C.\\nFine-Tuning vs. RAG: In the results of both the MMLU\\nand current events tasks, a significant advantage for RAG\\nover fine-tuning is evident. While fine-tuning improved\\nresults compared to the base model in most cases, it was not\\ncompetitive with the RAG approach.\\nSeveral factors might contribute to this behavior. Firstly,\\nRAG not only adds knowledge to a model but also incor-\\nporates context relevant to the question, a feature lacking\\nin fine-tuning. Additionally, fine-tuning may impact other\\ncapabilities of the model due to a degree of catastrophic for-\\ngetting. Finally, it’s plausible that unsupervised fine-tuned\\nmodels might benefit from further alignment through super-\\nvised or RL-based fine-tuning, as evidenced by the vastly'), Document(metadata={'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'creator': 'LaTeX with hyperref', 'total_pages': 14.0, 'page_label': '2', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'moddate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'source': '/content/2312.05934v3.pdf', 'producer': 'pdfTeX-1.40.25', 'page': 1.0, 'title': '', 'author': ''}, page_content='Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\\nbilities of LLMs through a comparison of fine-tuning and\\nRAG. To illustrate the rationale, let us use an analogy. Con-\\nsider three college students taking a test on a specific topic.\\nAll had access to class materials but didn’t know the topic\\nbeforehand. The first student had the textbook only during\\nthe test, the second had pre-test access and studied, and the\\nthird lost access upon the test announcement. Who would\\nprobably perform better?\\n2. Background\\nTo assess knowledge injection, we must first understand\\nwhat knowledge means for LLMs.\\nKnowledge and Language Models Defining knowledge\\nis a complex philosophical task far beyond the scope of this\\nresearch. However, we can examine what factual knowledge\\nmeans in the context of language models. If a model knows\\na fact, it can accurately and consistently answer questions\\nabout it. Furthermore, it can reliably distinguish between'), Document(metadata={'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'creator': 'LaTeX with hyperref', 'total_pages': 14.0, 'page_label': '8', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'moddate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'source': '/content/2312.05934v3.pdf', 'producer': 'pdfTeX-1.40.25', 'page': 7.0, 'title': '', 'author': ''}, page_content='Learning New Information In Figure 3, we can see an in-\\nteresting phenomenon observed throughout our experiments.\\nAfter each epoch, i.e., completing another iteration over the\\nentire dataset, the training loss drops significantly. This is\\nconsistent with what is known about LLMs memorizing the\\ndata during training and overfitting (Tirumala et al., 2022).\\nOur hypothesis is as follows:\\nIn order to teach pre-trained LLMs new knowl-\\nedge, the knowledge must be repeated in numer-\\nous ways.\\nThis is well known for LLM pre-training (Kandpal et al.,\\n2023), and we see in this case that this holds for fine-tuning\\nas well. The rationale for this hypothesis is that mere mem-\\norization of sentences does not entail knowledge of their\\ncontent, as was already shown in (Berglund et al., 2023). By\\nproviding the information in numerous forms (like the data\\naugmentation process we used), the various relationships in\\nthe data (e.g., a =⇒ b, b ̸=⇒ c) stand a higher chance')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(db.similarity_search(\"What is an RAG? Explain.\", k=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhBw5oq5QVvB",
        "outputId": "5ad454c7-031c-426e-d218-e831afe83fa3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'creator': 'LaTeX with hyperref', 'total_pages': 14.0, 'page_label': '7', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'moddate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'source': '/content/2312.05934v3.pdf', 'title': '', 'page': 6.0, 'producer': 'pdfTeX-1.40.25', 'author': ''}, page_content='calculation. More formally, this means that in Equation (1)\\nwe say that M(qn) = cn if:\\ncn = arg max\\nl\\n{M(qn∥a1\\nn), . . . ,M(qn∥aL\\nn )}, (4)\\nwhere M(qn∥al\\nn) = log PM(qn∥al\\nn).\\nMMLU Results For each task and model, we compared\\nfour approaches: using just the base model, RAG, FT, and fi-\\nnally combining FT and RAG by using the fine-tuned model\\nas the generator. Furthermore, we tested the MMLU tasks\\nusing both 0-shot and 5-shot scenarios. The full results are\\nshown in Table 1. An aggregation of the relative accuracy\\ngain, i.e.,\\n(LM′,Q − LM,Q)/LM,Q, (5)\\nwhere M is the base model and M′ is the knowledge-\\ninjected model, is shown in Figure 2.\\nIn all cases, RAG performed significantly better compared\\nto the base models. Furthermore, using RAG with the base\\nmodel as the generator was consistently better than only fine-\\ntuning. In some cases, using the fine-tuned model instead\\nof the base model as the generator in the RAG pipeline im-'), Document(metadata={'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'creator': 'LaTeX with hyperref', 'total_pages': 14.0, 'page_label': '1', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'source': '/content/2312.05934v3.pdf', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'moddate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'title': '', 'page': 0.0, 'producer': 'pdfTeX-1.40.25', 'author': ''}, page_content='RAG consistently outperforms it, both for exist-\\ning knowledge encountered during training and\\nentirely new knowledge. Moreover, we find that\\nLLMs struggle to learn new factual information\\nthrough unsupervised fine-tuning, and that expos-\\ning them to numerous variations of the same fact\\nduring training could alleviate this problem.\\nKeywords: LLMs, NLP, Fine-Tuning vs. RAG, Knowledge\\nand Factuality.\\n1. Introduction\\nLarge language models (LLMs) are able to capture vast\\namounts of factual information (Petroni et al., 2019; Cohen\\net al., 2023; Hu et al., 2023). LLMs exhibit a remarkable\\nlevel of knowledge in various domains due to their massive\\npre-training datasets. However, there are two significant\\nlimitations to this knowledge. First, it is static and does\\nnot update with time. Second, it is non-specific and thus\\n*Corresponding author.\\n†Equal contribution.\\nmay lack nuanced expertise in particular domains. While\\nthese are two different problems, they are deeply related'), Document(metadata={'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'creator': 'LaTeX with hyperref', 'total_pages': 14.0, 'page_label': '12', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2312.05934v3.pdf', 'moddate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'producer': 'pdfTeX-1.40.25', 'page': 11.0, 'title': '', 'author': ''}, page_content='Orca2 7B 0.648 0.645 0.660 0.670 0.679\\nPrehistory (5-shot)\\nMistral 7B 0.710 0.750 0.759 0.756 0.762\\nLlama2 7B 0.512 0.485 0.525 0.519 0.531\\nOrca2 7B 0.660 0.688 0.685 0.698 0.688\\nTable 3. RAG ablation study.\\nB. Paraphrase Examples\\nBelow is the prompt we used to generate paraphrases with GPT-4:\\n12'), Document(metadata={'subject': 'Proceedings of the International Conference on Machine Learning 2024', 'page_label': '7', 'total_pages': 14.0, 'creator': 'LaTeX with hyperref', 'keywords': '', 'trapped': '/False', 'creationdate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'moddate': datetime.datetime(2024, 1, 31, 1, 37, 51, tzinfo=datetime.timezone.utc), 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/2312.05934v3.pdf', 'producer': 'pdfTeX-1.40.25', 'page': 6.0, 'title': '', 'author': ''}, page_content='proved results even further. However, this is not consistent\\nand thus demonstrates the inherent instability of fine-tuning.\\nAdditionally, we found that the 5-shot approach boosts the\\nresults by a small margin in most cases, with a similar trend\\nbeing observed in all of the different approaches.\\nCurrent Events Results The evaluation on the current\\nevents task is shown in Table 2. RAG proves particularly\\neffective due to the one-to-one correspondence between\\nthe questions and the auxiliary dataset (see Section 4.3).\\nFine-tuning is not competitive with RAG. However, fine-\\ntuning with multiple paraphrases still provides a significant\\nimprovement over the baseline. We note that combining\\nRAG with fine-tuning shows inferior performance compared\\nto RAG alone.\\nIt is worth noting that although the questions are based on\\ninformation the models were not exposed to during training,\\nthe results of the base models surpass 1\\nL = 0.25. This can')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use ten sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "RoFNsH_5QqB0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkNGIVsXVUiR",
        "outputId": "25573119-911e-42bc-918d-94f4402cd654"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse ten sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HUGGINGFACEHUB_API_TOKEN = userdata.get('HUGGINGFACEHUB_API_TOKEN')"
      ],
      "metadata": {
        "id": "5tTBZvbjb6-j"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "llm_model = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
        "    temperature=1.0,\n",
        "    max_new_tokens=1000\n",
        ")\n",
        "\n",
        "model = ChatHuggingFace(llm=llm_model)"
      ],
      "metadata": {
        "id": "2GSJBaF6Xro-"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "retreiver = db.as_retriever()\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retreiver, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")"
      ],
      "metadata": {
        "id": "Ss1CC6freK59"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"Explain in breif, what is fine-tuning?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "V3iuP0zDf21z",
        "outputId": "692312de-e6b2-47af-9c92-ab8154a6bbb2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Fine-tuning is the process of adjusting a pre-trained model on a specific task or dataset to enhance its performance on that task by fine-tuning on that data, typically through supervised learning. It involves distinguishing between different types such as factual and non-factual questions. This technique can incorporate external knowledge during training or inference time, and can improve the model's performance on that set of questions. Fine-tuning algorithms are classified as either supervised or unsupervised. It compares two methods for knowledge injection, fine-tuning and retrieval augmentation (RAG), presented through consistent terminology in Equations 1 and 2, where knowledge is defined for language models in terms of answerable questions. Given text containing relevant information can improve language model's ability to answer questions about that knowledge. The model's performance on related facts is established by understanding what knowledge means for LLMs, but can distinguish between facts and questions through the model's ability for appropriate context in language models beyond the current scope of research. The approach can accurately and consistently answer related information, with an explanation through reasoning and existing facts in the context of language models. Additionally, fine-tuning may benefit from alignment through supervision or RL-based fine-tuning, as seen in the current results for MMLU tasks, showing an advantage in most cases for fine-tuning but not in unsupervised learning. The models without external data utilize reasoning and pre-existing knowledge for related questions during training or inference time relevant to the task, but the former may also enhance other abilities due to catastrophic forgetting. In the RAG comparison for MMLU and current events datasets, it's probable for RAG due to supervision in fine-tuning.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"What is a LLM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "8nMaoMVyi_eH",
        "outputId": "90054aff-ae1d-488e-f05e-9c2c3496177f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' LLM stands for Language Model, a type of artificial intelligence model that is trained on a large corpus of text with the goal of predicting the next word in a sequence given the previous words. Fine-tuning involves fine-tuning on a smaller dataset for a specific task, and knowledge injection refers to training an LLM with new information for a specific task. The ability for a model to answer questions about previously unseen facts is called factual knowledge injection. The propensity for memorization in LLMs during training is known, but interpreting knowledge requires context in its training data, shown by previous work. The selection of a clean dataset made specifically for evaluation aims to test LLM proficiency in each task. The measurement of knowledge in LLMs for specific tasks is called fine-tuning, and binary classification of cleaned examples is called knowledge injection for evaluation. Training and retrieval for specific tasks involves the repetition of information in its training data, highlighted in previous work. The difficulty for memorizing information is called fine-tuning for evaluation, and reliably distinguishing related facts is called factual knowledge injection in evaluation. The theory of LLMs involves knowledge injection for evaluation. The division into clean examples for specific tasks aims to test LLM proficiency in evaluation. The task for LLMs to answer previously unseen facts is called knowledge injection in evaluation. The novelty of LLMs in tasks aims to test LLM proficiency in isolation. The functionality in LLMs for specific tasks aims to contrast knowledge injection with evaluation. The measurement of knowledge for specific tasks aims to test LLM proficiency in contrast. The assumption for evaluating LLMs requires knowledge injection with cleaned examples. The novelty for tasks aims to evaluate LLM proficiency in evaluation. The consistency in LLMs for tasks aims to test fine-tuning for evaluation. The standard for tasks aims to evaluate knowledge injection for specific tasks. The capacity to answer previously unseen facts aims to evaluate LLM proficiency in evaluation. The designation for tasks aims to evaluate LLM proficiency in evaluation. The ability for tasks aims to test retrival for LLMs in evaluation. The strategy for tasks aims to compare fine-tuning with knowledge injection for evaluation. The novelty for LLMs aims to test knowledge injection for evaluation. The sophistication for tasks aims to evaluate LLM proficiency in evaluation. The aim for tasks aims to compare knowledge injection in evaluation. The standard for LLMs aims to test fine-tuning for evaluation. The ability for LLMs aims to compare fine-tuning with knowledge injection for evaluation. The future for tasks aims to evaluate LLM proficiency in evaluation. The selection for tasks aims to evaluate LLM proficiency in evaluation. The difficulty for tasks aims to contrast knowledge injection for evaluation. The principle for tasks aims to test retrieval for evaluation. The introduction for tasks aims to compare factual knowledge injection for evaluation. The goal for tasks aims to evaluate fine-tuning for evaluation. The significance for LLMs aims to contrast fine-tuning for evaluation. The necessity for tasks aims to test LLM proficiency in evaluation. The essence for tasks aims to contrast knowledge injection for evaluation. The contrast for LLMs aims to evaluate retrieval for evaluation, and the goal for tasks aims to contrast fine-tuning with failed examples for evaluation. The supposition for tasks aims to test LLM proficiency in evaluation. The model for tasks aims to compare LLM proficiency in evaluation. The aim for tasks aims to contrast knowledge injection for evaluation. The inclination for tasks aims to evaluate fine-tuning for evaluation. The preference for tasks aims to test LLM proficiency in evaluation. The ambition for tasks aims to contrast retrieval for evaluation. The significance for LLMs aims to test fine-tuning for evaluation. The novelty for tasks aims to contrast retrieval for evaluation. The example for tasks aims to evaluate fine-tuning for evaluation. The purpose for tasks aims to contrast LLM proficiency in evaluation. The contention for tasks aims to contrast knowledge injection for evaluation. The contrast for tasks aims to evaluate fine-tuning for evaluation. The purview for tasks aims to contrast LLM proficiency in evaluation. The guise for tasks aims to test knowledge injection for evaluation. The degree for tasks aims to contrast retrieval for evaluation. The contention for tasks aims to evaluate fine-tuning for evaluation. The necessity for tasks aims to test LLM proficiency in evaluation. The ultimatum for tasks aims to contrast knowledge injection for evaluation. The range for tasks aims to test fine-tuning for evaluation. The joystick for tasks aims to compare LLM proficiency in evaluation. The contention for tasks aims to contrast knowledge injection for evaluation. The relevance for tasks aims to test LLM proficiency in evaluation. The optimization for tasks aims to contrast retrieval for evaluation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    }
  ]
}